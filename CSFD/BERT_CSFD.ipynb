{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_CSFD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and metrics"
      ],
      "metadata": {
        "id": "G2OD8nmoZO2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7aHP4BG_PBW",
        "outputId": "948b763c-7504-4f41-8253-d16106a6d3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "\n",
        "import csv \n",
        "import numpy as np # numpy\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split # split for validation sets\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score # metrics\n",
        "import transformers as tr # transformers\n",
        "from transformers import ElectraForPreTraining, AutoTokenizer # tokenizers\n",
        "from datasets import load_metric, load_dataset # utility functions\n",
        "import torch\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "id": "jV_BHI5gZiAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sjSLh4gbHqAi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8wrGeiWKC5OO",
        "outputId": "9f841931-4c32-47a6-ec2d-7f1bd8af5306"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "train_data = pd.read_csv('train_data_csfd.csv')\n",
        "train_data = shuffle(train_data)\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label\n",
              "24669  Tyhle filmy jsou mnohem lep≈°√≠ ne≈æ ty z 21. sto...      1\n",
              "5879   Interstate 60 vytv√°≈ô√≠ koncept samostatn√©ho svƒõ...      1\n",
              "9553   Jestli je nƒõjak√Ω po≈ôad kter√Ω si zaslou≈æ√≠ odpad...      0\n",
              "3785   Dvojka u≈æ byla tak√© ≈ô√°dnƒõ vyva≈ôen√° a p≈ôesto se...      0\n",
              "18870                Nevtipn√Ω komik. Dƒõsiv√° kombinace 0%      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14f81c8d-9d18-41ba-971f-6332b0e1ac50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24669</th>\n",
              "      <td>Tyhle filmy jsou mnohem lep≈°√≠ ne≈æ ty z 21. sto...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5879</th>\n",
              "      <td>Interstate 60 vytv√°≈ô√≠ koncept samostatn√©ho svƒõ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9553</th>\n",
              "      <td>Jestli je nƒõjak√Ω po≈ôad kter√Ω si zaslou≈æ√≠ odpad...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3785</th>\n",
              "      <td>Dvojka u≈æ byla tak√© ≈ô√°dnƒõ vyva≈ôen√° a p≈ôesto se...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18870</th>\n",
              "      <td>Nevtipn√Ω komik. Dƒõsiv√° kombinace 0%</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14f81c8d-9d18-41ba-971f-6332b0e1ac50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-14f81c8d-9d18-41ba-971f-6332b0e1ac50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-14f81c8d-9d18-41ba-971f-6332b0e1ac50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWzdFujjJv8k"
      },
      "source": [
        "x_train_data = list(train_data['text'])\n",
        "# print(x[:5])\n",
        "\n",
        "y_train_data = list(train_data['label'])\n",
        "# print(y[:5])\n",
        "\n",
        "# x_train_data = x_train_data[:20000]\n",
        "# y_train_data = y_train_data[:20000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- uncomment to use other values"
      ],
      "metadata": {
        "id": "XZFhKnICl4YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('test_data_csfd.csv')\n",
        "\n",
        "x_test = list(test_data['text'])\n",
        "y_test = list(test_data['label'])"
      ],
      "metadata": {
        "id": "4cuC1ak6cLLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize data"
      ],
      "metadata": {
        "id": "m_Cs0QxEbGk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train_data, y_train_data, test_size=0.20, random_state=69)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Seznam/small-e-czech\")\n",
        "train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=50)\n",
        "val_encodings = tokenizer(x_val, truncation=True, padding=True, max_length=50)\n",
        "test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=50)"
      ],
      "metadata": {
        "id": "mHvJmLfSSHOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93451d39-c2d2-441c-ce8a-728da447cc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/Seznam/small-e-czech/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5757afa75f84a9d8eae0cab2cc71caa7fa734ce29a3d1460d8ff138877a6272c.b4542522909d7c3467d6bb947216c79cef9ed619c6fb7c1b732d20e768f0c674\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"Seznam/small-e-czech\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/Seznam/small-e-czech/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/40b75ee15c32fd2d05e334eea76d5bd298adeefd87f42cfb86ac839d45592c81.d26e564cea92d390c578d48a1a11a321dfd983a30fd91eaa1d09314994036e4e\n",
            "loading file https://huggingface.co/Seznam/small-e-czech/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/Seznam/small-e-czech/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/Seznam/small-e-czech/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/Seznam/small-e-czech/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d5c7e710f84af97c5c5d13c0f70b1a0465f94643dbdcbaf76bc3b2c7c44cd536.146bfb67daecf8765f24ebbf4b9507e46e486d30038629a82742d923a03d6940\n",
            "loading configuration file https://huggingface.co/Seznam/small-e-czech/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5757afa75f84a9d8eae0cab2cc71caa7fa734ce29a3d1460d8ff138877a6272c.b4542522909d7c3467d6bb947216c79cef9ed619c6fb7c1b732d20e768f0c674\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"Seznam/small-e-czech\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/Seznam/small-e-czech/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5757afa75f84a9d8eae0cab2cc71caa7fa734ce29a3d1460d8ff138877a6272c.b4542522909d7c3467d6bb947216c79cef9ed619c6fb7c1b732d20e768f0c674\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"Seznam/small-e-czech\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PrepareDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = PrepareDataset(train_encodings, y_train)\n",
        "val_dataset = PrepareDataset(val_encodings, y_val)\n",
        "test_dataset = PrepareDataset(test_encodings, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "ih3KT0_6F0Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small-e-czech"
      ],
      "metadata": {
        "id": "KYtQTL5Fbw0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Experiment"
      ],
      "metadata": {
        "id": "DY6pEFq9b0WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, ElectraForSequenceClassification, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps = 4000,\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=1,  # batch size per device during training\n",
        "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
        "    warmup_steps=2500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    #load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Seznam/small-e-czech\", num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iDhvC3VFGUC2",
        "outputId": "3fcdc2e8-a212-4ee7-9136-a2ff6ee26d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/Seznam/small-e-czech/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5757afa75f84a9d8eae0cab2cc71caa7fa734ce29a3d1460d8ff138877a6272c.b4542522909d7c3467d6bb947216c79cef9ed619c6fb7c1b732d20e768f0c674\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"Seznam/small-e-czech\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/Seznam/small-e-czech/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dbbba7b71c98d7a40c56956cd64d8648d21cc4bc38afb5156be6e182a9764db7.4915169c014b07729d6f31313b2615c96a80a7c049b8c938bf113ebba410dbed\n",
            "Some weights of the model checkpoint at Seznam/small-e-czech were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Seznam/small-e-czech and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 16000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16000' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16000/16000 31:05, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.873100</td>\n",
              "      <td>0.838835</td>\n",
              "      <td>0.823400</td>\n",
              "      <td>0.803605</td>\n",
              "      <td>0.856000</td>\n",
              "      <td>0.828975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.724200</td>\n",
              "      <td>0.979783</td>\n",
              "      <td>0.798000</td>\n",
              "      <td>0.944511</td>\n",
              "      <td>0.633200</td>\n",
              "      <td>0.758142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.640900</td>\n",
              "      <td>0.736621</td>\n",
              "      <td>0.858500</td>\n",
              "      <td>0.863812</td>\n",
              "      <td>0.851200</td>\n",
              "      <td>0.857459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.799200</td>\n",
              "      <td>0.729353</td>\n",
              "      <td>0.863000</td>\n",
              "      <td>0.862420</td>\n",
              "      <td>0.863800</td>\n",
              "      <td>0.863110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-6500\n",
            "Configuration saved in ./results/checkpoint-6500/config.json\n",
            "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-7000\n",
            "Configuration saved in ./results/checkpoint-7000/config.json\n",
            "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-7500\n",
            "Configuration saved in ./results/checkpoint-7500/config.json\n",
            "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/checkpoint-8000\n",
            "Configuration saved in ./results/checkpoint-8000/config.json\n",
            "Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-8500\n",
            "Configuration saved in ./results/checkpoint-8500/config.json\n",
            "Model weights saved in ./results/checkpoint-8500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-9000\n",
            "Configuration saved in ./results/checkpoint-9000/config.json\n",
            "Model weights saved in ./results/checkpoint-9000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-9500\n",
            "Configuration saved in ./results/checkpoint-9500/config.json\n",
            "Model weights saved in ./results/checkpoint-9500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-10000\n",
            "Configuration saved in ./results/checkpoint-10000/config.json\n",
            "Model weights saved in ./results/checkpoint-10000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-10500\n",
            "Configuration saved in ./results/checkpoint-10500/config.json\n",
            "Model weights saved in ./results/checkpoint-10500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-11000\n",
            "Configuration saved in ./results/checkpoint-11000/config.json\n",
            "Model weights saved in ./results/checkpoint-11000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-11500\n",
            "Configuration saved in ./results/checkpoint-11500/config.json\n",
            "Model weights saved in ./results/checkpoint-11500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/checkpoint-12000\n",
            "Configuration saved in ./results/checkpoint-12000/config.json\n",
            "Model weights saved in ./results/checkpoint-12000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-12500\n",
            "Configuration saved in ./results/checkpoint-12500/config.json\n",
            "Model weights saved in ./results/checkpoint-12500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-13000\n",
            "Configuration saved in ./results/checkpoint-13000/config.json\n",
            "Model weights saved in ./results/checkpoint-13000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-13500\n",
            "Configuration saved in ./results/checkpoint-13500/config.json\n",
            "Model weights saved in ./results/checkpoint-13500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-14000\n",
            "Configuration saved in ./results/checkpoint-14000/config.json\n",
            "Model weights saved in ./results/checkpoint-14000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-14500\n",
            "Configuration saved in ./results/checkpoint-14500/config.json\n",
            "Model weights saved in ./results/checkpoint-14500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-15000\n",
            "Configuration saved in ./results/checkpoint-15000/config.json\n",
            "Model weights saved in ./results/checkpoint-15000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-15500\n",
            "Configuration saved in ./results/checkpoint-15500/config.json\n",
            "Model weights saved in ./results/checkpoint-15500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/checkpoint-16000\n",
            "Configuration saved in ./results/checkpoint-16000/config.json\n",
            "Model weights saved in ./results/checkpoint-16000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=16000, training_loss=0.7725331554412842, metrics={'train_runtime': 1865.2655, 'train_samples_per_second': 8.578, 'train_steps_per_second': 8.578, 'total_flos': 45968188800000.0, 'train_loss': 0.7725331554412842, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "AtvGvsnccET3",
        "outputId": "27382178-dd92-45b2-9056-739c7e3554a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 02:22]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-3.3468058,  3.6649668],\n",
              "       [ 3.0012815, -3.4552958],\n",
              "       [ 2.9640367, -3.4117818],\n",
              "       ...,\n",
              "       [ 2.426494 , -2.7394288],\n",
              "       [-3.307341 ,  3.6172903],\n",
              "       [-2.2921898,  2.4988637]], dtype=float32), label_ids=array([1, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.7293527126312256, 'test_accuracy': 0.863, 'test_precision': 0.8624201277955271, 'test_recall': 0.8638, 'test_f1': 0.8631095123900879, 'test_runtime': 142.7206, 'test_samples_per_second': 70.067, 'test_steps_per_second': 70.067})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Experiment"
      ],
      "metadata": {
        "id": "bG0l_GJWcEcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, ElectraForSequenceClassification, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps = 10000,\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=1,  # batch size per device during training\n",
        "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
        "    warmup_steps=5000,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    #load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Seznam/small-e-czech\", num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0iDuZwZ0cQt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "1TQ9pYvjqBis",
        "outputId": "046a14fe-9104-494a-bd57-d69487519227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 02:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[ 3.7413886, -3.9276087],\n",
              "       [ 2.7627022, -2.8717153],\n",
              "       [ 3.763007 , -3.9434583],\n",
              "       ...,\n",
              "       [ 3.7326827, -3.9196742],\n",
              "       [ 3.7301762, -3.901807 ],\n",
              "       [-2.7818615,  2.990993 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 1]), metrics={'test_loss': 0.6246399879455566, 'test_accuracy': 0.8908, 'test_precision': 0.8710596278009874, 'test_recall': 0.9174, 'test_f1': 0.8936294564582116, 'test_runtime': 144.2598, 'test_samples_per_second': 69.319, 'test_steps_per_second': 69.319})"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Experiment"
      ],
      "metadata": {
        "id": "OrjE_ASYlfUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, ElectraForSequenceClassification, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps = 500,\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Seznam/small-e-czech\", num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79ade69b-6451-45c8-d19a-af8fcf4b4bac",
        "id": "lV1bnj6dliSX"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/Seznam/small-e-czech/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5757afa75f84a9d8eae0cab2cc71caa7fa734ce29a3d1460d8ff138877a6272c.b4542522909d7c3467d6bb947216c79cef9ed619c6fb7c1b732d20e768f0c674\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"Seznam/small-e-czech\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/Seznam/small-e-czech/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dbbba7b71c98d7a40c56956cd64d8648d21cc4bc38afb5156be6e182a9764db7.4915169c014b07729d6f31313b2615c96a80a7c049b8c938bf113ebba410dbed\n",
            "Some weights of the model checkpoint at Seznam/small-e-czech were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Seznam/small-e-czech and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 40490\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7593\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5501' max='7593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5501/7593 16:26 < 06:15, 5.58 it/s, Epoch 2.17/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.590100</td>\n",
              "      <td>0.431672</td>\n",
              "      <td>0.816200</td>\n",
              "      <td>0.784046</td>\n",
              "      <td>0.872800</td>\n",
              "      <td>0.826046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.385900</td>\n",
              "      <td>0.338738</td>\n",
              "      <td>0.854600</td>\n",
              "      <td>0.822716</td>\n",
              "      <td>0.904000</td>\n",
              "      <td>0.861445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.328200</td>\n",
              "      <td>0.315029</td>\n",
              "      <td>0.869700</td>\n",
              "      <td>0.847136</td>\n",
              "      <td>0.902200</td>\n",
              "      <td>0.873801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.319400</td>\n",
              "      <td>0.280348</td>\n",
              "      <td>0.880200</td>\n",
              "      <td>0.871870</td>\n",
              "      <td>0.891400</td>\n",
              "      <td>0.881527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.261613</td>\n",
              "      <td>0.887800</td>\n",
              "      <td>0.872885</td>\n",
              "      <td>0.907800</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.299452</td>\n",
              "      <td>0.883500</td>\n",
              "      <td>0.851384</td>\n",
              "      <td>0.929200</td>\n",
              "      <td>0.888591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.230500</td>\n",
              "      <td>0.298726</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>0.864678</td>\n",
              "      <td>0.927800</td>\n",
              "      <td>0.895128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.219900</td>\n",
              "      <td>0.313763</td>\n",
              "      <td>0.886800</td>\n",
              "      <td>0.848092</td>\n",
              "      <td>0.942400</td>\n",
              "      <td>0.892762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.231600</td>\n",
              "      <td>0.302360</td>\n",
              "      <td>0.882300</td>\n",
              "      <td>0.834588</td>\n",
              "      <td>0.953600</td>\n",
              "      <td>0.890133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.216100</td>\n",
              "      <td>0.267262</td>\n",
              "      <td>0.895500</td>\n",
              "      <td>0.866815</td>\n",
              "      <td>0.934600</td>\n",
              "      <td>0.899432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='416' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [416/625 00:14 < 00:07, 28.10 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7593' max='7593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7593/7593 23:05, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.590100</td>\n",
              "      <td>0.431672</td>\n",
              "      <td>0.816200</td>\n",
              "      <td>0.784046</td>\n",
              "      <td>0.872800</td>\n",
              "      <td>0.826046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.385900</td>\n",
              "      <td>0.338738</td>\n",
              "      <td>0.854600</td>\n",
              "      <td>0.822716</td>\n",
              "      <td>0.904000</td>\n",
              "      <td>0.861445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.328200</td>\n",
              "      <td>0.315029</td>\n",
              "      <td>0.869700</td>\n",
              "      <td>0.847136</td>\n",
              "      <td>0.902200</td>\n",
              "      <td>0.873801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.319400</td>\n",
              "      <td>0.280348</td>\n",
              "      <td>0.880200</td>\n",
              "      <td>0.871870</td>\n",
              "      <td>0.891400</td>\n",
              "      <td>0.881527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.261613</td>\n",
              "      <td>0.887800</td>\n",
              "      <td>0.872885</td>\n",
              "      <td>0.907800</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.299452</td>\n",
              "      <td>0.883500</td>\n",
              "      <td>0.851384</td>\n",
              "      <td>0.929200</td>\n",
              "      <td>0.888591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.230500</td>\n",
              "      <td>0.298726</td>\n",
              "      <td>0.891300</td>\n",
              "      <td>0.864678</td>\n",
              "      <td>0.927800</td>\n",
              "      <td>0.895128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.219900</td>\n",
              "      <td>0.313763</td>\n",
              "      <td>0.886800</td>\n",
              "      <td>0.848092</td>\n",
              "      <td>0.942400</td>\n",
              "      <td>0.892762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.231600</td>\n",
              "      <td>0.302360</td>\n",
              "      <td>0.882300</td>\n",
              "      <td>0.834588</td>\n",
              "      <td>0.953600</td>\n",
              "      <td>0.890133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.216100</td>\n",
              "      <td>0.267262</td>\n",
              "      <td>0.895500</td>\n",
              "      <td>0.866815</td>\n",
              "      <td>0.934600</td>\n",
              "      <td>0.899432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.184800</td>\n",
              "      <td>0.316995</td>\n",
              "      <td>0.891700</td>\n",
              "      <td>0.858109</td>\n",
              "      <td>0.938600</td>\n",
              "      <td>0.896552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.167000</td>\n",
              "      <td>0.340492</td>\n",
              "      <td>0.894100</td>\n",
              "      <td>0.860567</td>\n",
              "      <td>0.940600</td>\n",
              "      <td>0.898806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.162800</td>\n",
              "      <td>0.401751</td>\n",
              "      <td>0.881400</td>\n",
              "      <td>0.831422</td>\n",
              "      <td>0.956800</td>\n",
              "      <td>0.889715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.168300</td>\n",
              "      <td>0.368202</td>\n",
              "      <td>0.887600</td>\n",
              "      <td>0.844840</td>\n",
              "      <td>0.949600</td>\n",
              "      <td>0.894162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.378003</td>\n",
              "      <td>0.889400</td>\n",
              "      <td>0.848925</td>\n",
              "      <td>0.947400</td>\n",
              "      <td>0.895463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-6500\n",
            "Configuration saved in ./results/checkpoint-6500/config.json\n",
            "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-7000\n",
            "Configuration saved in ./results/checkpoint-7000/config.json\n",
            "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-7500\n",
            "Configuration saved in ./results/checkpoint-7500/config.json\n",
            "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-2500 (score: 0.26161304116249084).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7593, training_loss=0.25824740864645584, metrics={'train_runtime': 1385.3549, 'train_samples_per_second': 87.682, 'train_steps_per_second': 5.481, 'total_flos': 348984743346000.0, 'train_loss': 0.25824740864645584, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "b8018ae1-687b-4935-809e-cf4803791778",
        "id": "Si5_JdnTliSY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:22]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-2.1884203 ,  2.3354883 ],\n",
              "       [ 1.9350162 , -2.2224708 ],\n",
              "       [ 2.1401632 , -2.44996   ],\n",
              "       ...,\n",
              "       [-0.09943626,  0.11052746],\n",
              "       [-1.0771567 ,  1.1682254 ],\n",
              "       [-0.67679715,  0.7417249 ]], dtype=float32), label_ids=array([1, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.26161304116249084, 'test_accuracy': 0.8878, 'test_precision': 0.8728846153846154, 'test_recall': 0.9078, 'test_f1': 0.89, 'test_runtime': 22.2729, 'test_samples_per_second': 448.976, 'test_steps_per_second': 28.061})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Experiment"
      ],
      "metadata": {
        "id": "p2Z2uWb4uZUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, ElectraForSequenceClassification, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps = 500,\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Seznam/small-e-czech\", num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "187387be-c156-4ac3-fd11-e33c35bf3e5d",
        "id": "Me3U6Myaub8F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/Seznam/small-e-czech/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5757afa75f84a9d8eae0cab2cc71caa7fa734ce29a3d1460d8ff138877a6272c.b4542522909d7c3467d6bb947216c79cef9ed619c6fb7c1b732d20e768f0c674\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"Seznam/small-e-czech\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/Seznam/small-e-czech/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dbbba7b71c98d7a40c56956cd64d8648d21cc4bc38afb5156be6e182a9764db7.4915169c014b07729d6f31313b2615c96a80a7c049b8c938bf113ebba410dbed\n",
            "Some weights of the model checkpoint at Seznam/small-e-czech were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Seznam/small-e-czech and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 40490\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3798\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3798' max='3798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3798/3798 14:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.562000</td>\n",
              "      <td>0.412734</td>\n",
              "      <td>0.822900</td>\n",
              "      <td>0.772122</td>\n",
              "      <td>0.916200</td>\n",
              "      <td>0.838013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.350400</td>\n",
              "      <td>0.306464</td>\n",
              "      <td>0.867200</td>\n",
              "      <td>0.839121</td>\n",
              "      <td>0.908600</td>\n",
              "      <td>0.872479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.288200</td>\n",
              "      <td>0.324021</td>\n",
              "      <td>0.864500</td>\n",
              "      <td>0.815148</td>\n",
              "      <td>0.942800</td>\n",
              "      <td>0.874339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.241400</td>\n",
              "      <td>0.258393</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.865579</td>\n",
              "      <td>0.923400</td>\n",
              "      <td>0.893555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.232700</td>\n",
              "      <td>0.251818</td>\n",
              "      <td>0.895500</td>\n",
              "      <td>0.875166</td>\n",
              "      <td>0.922600</td>\n",
              "      <td>0.898257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.188100</td>\n",
              "      <td>0.316629</td>\n",
              "      <td>0.883700</td>\n",
              "      <td>0.840582</td>\n",
              "      <td>0.947000</td>\n",
              "      <td>0.890624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.175900</td>\n",
              "      <td>0.306419</td>\n",
              "      <td>0.881400</td>\n",
              "      <td>0.835385</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.889014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-2500 (score: 0.2518179714679718).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3798, training_loss=0.28147689614439086, metrics={'train_runtime': 855.7392, 'train_samples_per_second': 141.947, 'train_steps_per_second': 4.438, 'total_flos': 348984743346000.0, 'train_loss': 0.28147689614439086, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "925ebbcf-353a-4378-f664-eee9938531d5",
        "id": "BeHhFCbSub8F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:17]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-1.9429344 ,  2.6439602 ],\n",
              "       [ 1.7200942 , -2.2398076 ],\n",
              "       [ 2.251006  , -2.9004934 ],\n",
              "       ...,\n",
              "       [ 0.34885728, -0.45480007],\n",
              "       [-1.3957477 ,  1.9016438 ],\n",
              "       [-1.2087216 ,  1.6491921 ]], dtype=float32), label_ids=array([1, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.2518179714679718, 'test_accuracy': 0.8955, 'test_precision': 0.8751660026560425, 'test_recall': 0.9226, 'test_f1': 0.8982572290916171, 'test_runtime': 17.6342, 'test_samples_per_second': 567.078, 'test_steps_per_second': 17.75})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model"
      ],
      "metadata": {
        "id": "VovUMEsxcJ0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'modelfirstCZ')"
      ],
      "metadata": {
        "id": "Qmq_B27yqCLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on my own data"
      ],
      "metadata": {
        "id": "z4QCfvk3cAVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing it on my own data\n",
        "df = pd.read_fwf('my_test.txt', delimeter='\\n', header=None)\n",
        "my_data = pd.DataFrame({'text':df[0], 'label':1})\n",
        "my_data[1:3].label = 0\n",
        "my_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "KDXDZZJE8Xl7",
        "outputId": "30f05f78-5ab7-45fc-b64b-1af3df79d431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d577d5a0-b913-4f31-a365-017051997789\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Film byl skvƒõl√Ω.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Nedoporuƒçuji, nestalo to za nic, ztr√°ta ƒçasu.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Stra≈°nƒõ mƒõ bavilo koukat na 2 hodiny o niƒçem.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bylo to jako m√© studium na gymn√°ziu, pln√© v√Ωzev,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d577d5a0-b913-4f31-a365-017051997789')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d577d5a0-b913-4f31-a365-017051997789 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d577d5a0-b913-4f31-a365-017051997789');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               text  label\n",
              "0                                  Film byl skvƒõl√Ω.      1\n",
              "1     Nedoporuƒçuji, nestalo to za nic, ztr√°ta ƒçasu.      0\n",
              "2     Stra≈°nƒõ mƒõ bavilo koukat na 2 hodiny o niƒçem.      0\n",
              "3  Bylo to jako m√© studium na gymn√°ziu, pln√© v√Ωzev,      1"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_data_x = list(my_data['text'])\n",
        "\n",
        "my_label_y = list(my_data['label'])"
      ],
      "metadata": {
        "id": "i2C51TZ0814X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_encodings = tokenizer(my_data_x, truncation=True, padding=\"max_length\",  max_length=50)\n",
        "my_dataset = PrepareDataset(my_encodings, my_label_y)"
      ],
      "metadata": {
        "id": "QstgeV6X8_eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = trainer.predict(my_dataset)\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "outputs\n",
        "y_pred\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "dlr7Ro6YuOBM",
        "outputId": "897a2c47-af89-4622-c6fa-1fa68987cc2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 4\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22374' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 1:27:40]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    }
  ]
}